{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyspark import SparkContext\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "mypath = \"../../project/cmsc25025/sou/text\"  #Working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a list of all the files in the directory that are speeches\n",
    "NO_OPEN = ['soumeta.txt', 'speeches.json']\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f)) if f not in NO_OPEN]\n",
    "year_speech = [[f[:-9], int(f[-8:-4]), open(mypath + \"/\" + f).read()] for f in onlyfiles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Friends, Romans, countrymen, lend me your vectors\n",
    "Use the classic vector space model from IR to find similar SOU addresses.\n",
    "\n",
    "### a) Compute the TF-IDF vectors for each SOU address. You should\n",
    "- lower case all text\n",
    "- remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize a SparkContext\n",
    "sc = SparkContext(\"local\", \"Simple App\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load speeches to SparkContext and sort by year\n",
    "speeches = sc.parallelize(year_speech).sortBy(lambda tup: tup[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create RCC with [President, year, speech]\n",
    "sentences = speeches.map(lambda x: [x[0], x[1], x[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define punctuation to strip from documents\n",
    "PUNCTUATION = '.,:;()-\"[]{}' + \"'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Strip punctuation and lowercase \n",
    "words_stripped = sentences.map(lambda x: x[2].lower().translate(None, PUNCTUATION).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This will give us some (sparse) vectors for every document. The vectors are of size 2^16=65,536. This number\n",
    "# defaults to 2^20 but when trying to do k-means clustering, jupyter runs out of memory. It could even be restricted\n",
    "# to a smaller size.\n",
    "hashingTF = HashingTF(numFeatures=65536)\n",
    "tf = hashingTF.transform(words_stripped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(65536, {83: 1.0, 351: 2.0, 379: 2.0, 553: 1.0, 730: 1.0, 755: 1.0, 929: 1.0, 950: 1.0, 1223: 1.0, 1243: 2.0, 1257: 4.0, 1320: 3.0, 1331: 1.0, 1388: 1.0, 1580: 1.0, 1595: 9.0, 1750: 1.0, 1809: 1.0, 1845: 1.0, 1890: 1.0, 2284: 1.0, 2548: 1.0, 2590: 1.0, 2630: 1.0, 2683: 1.0, 2784: 1.0, 3071: 1.0, 3225: 1.0, 3245: 3.0, 3305: 1.0, 3421: 1.0, 3449: 2.0, 3592: 2.0, 3714: 1.0, 3873: 1.0, 3932: 15.0, 4006: 1.0, 4119: 2.0, 4145: 49.0, 4164: 1.0, 4291: 1.0, 4415: 10.0, 4514: 1.0, 4729: 1.0, 4730: 1.0, 4918: 1.0, 5009: 1.0, 5041: 1.0, 5348: 2.0, 5747: 1.0, 5753: 2.0, 5768: 2.0, 5898: 2.0, 5981: 1.0, 5998: 8.0, 6082: 1.0, 6115: 1.0, 6137: 1.0, 6206: 2.0, 6318: 1.0, 6434: 1.0, 6536: 1.0, 6890: 2.0, 6957: 2.0, 7199: 1.0, 7215: 2.0, 7237: 1.0, 7239: 4.0, 7289: 1.0, 7751: 3.0, 7801: 1.0, 8038: 1.0, 8075: 1.0, 8096: 1.0, 8131: 1.0, 8204: 1.0, 8220: 1.0, 8403: 2.0, 8822: 1.0, 8935: 1.0, 8966: 1.0, 8982: 1.0, 9112: 1.0, 9130: 1.0, 9139: 1.0, 9245: 1.0, 9343: 2.0, 9398: 1.0, 9410: 2.0, 9412: 1.0, 9593: 1.0, 9779: 2.0, 9822: 1.0, 9860: 1.0, 9946: 2.0, 10001: 1.0, 10045: 3.0, 10139: 1.0, 10415: 1.0, 10613: 4.0, 10664: 1.0, 10707: 1.0, 10840: 9.0, 10843: 2.0, 10903: 1.0, 10914: 1.0, 10928: 1.0, 11134: 1.0, 11210: 1.0, 11314: 1.0, 11624: 6.0, 11790: 3.0, 11811: 1.0, 11862: 1.0, 12330: 1.0, 12374: 1.0, 12600: 1.0, 12624: 1.0, 12975: 1.0, 12980: 1.0, 13042: 1.0, 13179: 1.0, 13265: 1.0, 13352: 1.0, 13444: 1.0, 13528: 1.0, 13590: 1.0, 13652: 1.0, 13835: 2.0, 14257: 1.0, 14397: 1.0, 14772: 1.0, 14775: 1.0, 14981: 1.0, 15036: 1.0, 15214: 1.0, 15283: 1.0, 15438: 1.0, 15514: 1.0, 15543: 1.0, 15795: 1.0, 15797: 1.0, 15852: 4.0, 15959: 1.0, 15972: 1.0, 16025: 1.0, 16130: 1.0, 16136: 1.0, 16140: 1.0, 16214: 1.0, 16290: 1.0, 16348: 1.0, 16384: 1.0, 16505: 1.0, 16508: 1.0, 16587: 1.0, 16625: 2.0, 16723: 1.0, 16857: 1.0, 16927: 2.0, 17021: 1.0, 17058: 1.0, 17095: 1.0, 17242: 3.0, 17308: 1.0, 17363: 1.0, 17419: 1.0, 17567: 1.0, 17646: 2.0, 17718: 2.0, 17834: 2.0, 17891: 1.0, 17949: 2.0, 18622: 1.0, 18698: 1.0, 18875: 1.0, 18888: 6.0, 18985: 1.0, 19000: 1.0, 19197: 1.0, 19198: 1.0, 19511: 1.0, 19713: 1.0, 19744: 1.0, 19745: 1.0, 19751: 1.0, 20085: 1.0, 20221: 1.0, 20237: 1.0, 20367: 2.0, 20385: 1.0, 20496: 1.0, 20617: 1.0, 20669: 5.0, 20907: 1.0, 21012: 1.0, 21191: 1.0, 21234: 1.0, 21294: 1.0, 21331: 3.0, 21457: 3.0, 21493: 1.0, 21579: 1.0, 21743: 2.0, 21877: 1.0, 22587: 1.0, 22718: 1.0, 22739: 3.0, 22781: 1.0, 23109: 1.0, 23370: 1.0, 23446: 14.0, 23517: 1.0, 23644: 1.0, 23839: 1.0, 24114: 2.0, 24119: 1.0, 24126: 1.0, 24166: 1.0, 24206: 1.0, 24292: 1.0, 24398: 1.0, 24631: 2.0, 24810: 1.0, 24827: 1.0, 25266: 1.0, 25397: 1.0, 25558: 1.0, 25851: 1.0, 25966: 2.0, 26216: 1.0, 26333: 1.0, 26551: 1.0, 26676: 1.0, 26735: 1.0, 26970: 1.0, 27380: 1.0, 27430: 2.0, 27456: 1.0, 27816: 1.0, 27904: 1.0, 27920: 1.0, 28000: 2.0, 28113: 1.0, 28149: 1.0, 28155: 2.0, 28204: 2.0, 28674: 3.0, 28834: 1.0, 28868: 1.0, 28965: 1.0, 28988: 1.0, 29016: 1.0, 29116: 1.0, 29432: 1.0, 29448: 1.0, 29899: 1.0, 29972: 1.0, 30017: 1.0, 30039: 1.0, 30333: 1.0, 30452: 3.0, 30545: 1.0, 30635: 1.0, 30815: 1.0, 31012: 3.0, 31161: 1.0, 31249: 2.0, 31311: 1.0, 31385: 17.0, 31391: 1.0, 31399: 1.0, 31417: 1.0, 31529: 1.0, 31537: 14.0, 31589: 1.0, 31670: 22.0, 31721: 1.0, 32094: 4.0, 32140: 1.0, 32174: 1.0, 32326: 1.0, 32370: 1.0, 32485: 1.0, 32611: 7.0, 32616: 2.0, 32974: 1.0, 33096: 1.0, 33201: 1.0, 33489: 1.0, 33615: 2.0, 33647: 1.0, 33735: 2.0, 33739: 2.0, 33944: 2.0, 33950: 1.0, 33984: 1.0, 34145: 4.0, 34229: 1.0, 34470: 16.0, 34539: 4.0, 34623: 1.0, 34631: 1.0, 34807: 1.0, 34906: 1.0, 35034: 1.0, 35076: 1.0, 35144: 1.0, 35289: 4.0, 35345: 1.0, 35506: 1.0, 36073: 1.0, 36149: 1.0, 36588: 1.0, 36595: 1.0, 36599: 1.0, 36631: 1.0, 36745: 3.0, 36748: 1.0, 36751: 6.0, 36754: 19.0, 36757: 4.0, 36806: 2.0, 36869: 1.0, 37264: 3.0, 37281: 2.0, 37391: 1.0, 37405: 4.0, 37623: 2.0, 37645: 1.0, 37712: 2.0, 37907: 2.0, 37960: 1.0, 38038: 1.0, 38080: 2.0, 38114: 1.0, 38150: 1.0, 38154: 1.0, 38225: 1.0, 38513: 1.0, 38593: 18.0, 38621: 18.0, 38954: 1.0, 39052: 1.0, 39124: 1.0, 39138: 4.0, 39219: 1.0, 39272: 8.0, 39309: 1.0, 39356: 2.0, 39414: 1.0, 39580: 1.0, 39712: 1.0, 39998: 2.0, 40120: 1.0, 40455: 3.0, 40456: 1.0, 40728: 1.0, 40756: 1.0, 41121: 3.0, 41221: 1.0, 41259: 1.0, 41402: 1.0, 41429: 1.0, 41435: 2.0, 41478: 1.0, 41538: 3.0, 41558: 3.0, 41611: 2.0, 41852: 3.0, 42039: 1.0, 42049: 1.0, 42209: 1.0, 42446: 1.0, 42490: 122.0, 42618: 1.0, 42643: 1.0, 42709: 1.0, 42732: 1.0, 42762: 1.0, 42802: 1.0, 43195: 1.0, 43229: 1.0, 43278: 1.0, 43427: 1.0, 43599: 1.0, 43612: 1.0, 43685: 1.0, 43740: 1.0, 43795: 1.0, 43847: 17.0, 43869: 2.0, 43895: 1.0, 44191: 1.0, 44313: 1.0, 44464: 2.0, 44635: 2.0, 44742: 1.0, 44791: 1.0, 44829: 7.0, 44986: 13.0, 45128: 2.0, 45223: 1.0, 45536: 21.0, 45723: 1.0, 45725: 1.0, 45739: 1.0, 45784: 1.0, 45975: 1.0, 46183: 1.0, 46229: 1.0, 46305: 3.0, 46350: 1.0, 46459: 1.0, 46732: 4.0, 46813: 1.0, 46817: 1.0, 46899: 1.0, 47100: 1.0, 47128: 1.0, 47240: 1.0, 47433: 1.0, 47484: 1.0, 47733: 1.0, 47760: 1.0, 47767: 2.0, 47912: 1.0, 48044: 1.0, 48055: 1.0, 48306: 2.0, 48328: 1.0, 48589: 2.0, 48611: 1.0, 48616: 8.0, 48892: 1.0, 49284: 1.0, 49312: 1.0, 49346: 1.0, 49368: 2.0, 49399: 1.0, 49460: 1.0, 49463: 1.0, 49511: 1.0, 49543: 1.0, 49751: 1.0, 50142: 1.0, 50299: 1.0, 50436: 1.0, 50479: 1.0, 50546: 1.0, 50570: 11.0, 50573: 22.0, 50583: 27.0, 50591: 1.0, 51047: 1.0, 51227: 2.0, 51396: 1.0, 51493: 1.0, 51576: 1.0, 51619: 1.0, 51692: 1.0, 51790: 1.0, 51908: 1.0, 51972: 1.0, 51992: 1.0, 52087: 2.0, 52224: 1.0, 52273: 1.0, 52574: 3.0, 52654: 1.0, 52915: 1.0, 53135: 2.0, 53401: 3.0, 53527: 2.0, 53544: 1.0, 53591: 2.0, 53808: 1.0, 53827: 1.0, 53859: 1.0, 53897: 3.0, 54070: 1.0, 54101: 1.0, 54235: 1.0, 54276: 3.0, 54588: 1.0, 54675: 1.0, 54924: 1.0, 54941: 1.0, 54986: 1.0, 55136: 1.0, 55175: 1.0, 55511: 1.0, 55733: 2.0, 55743: 1.0, 55825: 1.0, 56083: 1.0, 56358: 1.0, 56458: 4.0, 56505: 1.0, 56748: 1.0, 56909: 1.0, 57075: 1.0, 57091: 1.0, 57106: 1.0, 57166: 2.0, 57169: 1.0, 57419: 1.0, 57428: 1.0, 57696: 4.0, 57700: 1.0, 57724: 3.0, 58115: 1.0, 58197: 2.0, 58267: 1.0, 58373: 1.0, 58471: 1.0, 58670: 10.0, 58720: 1.0, 58777: 1.0, 58857: 1.0, 58901: 1.0, 59138: 1.0, 59303: 1.0, 59385: 1.0, 59462: 1.0, 59557: 1.0, 59644: 1.0, 59719: 1.0, 59913: 1.0, 60099: 1.0, 60215: 2.0, 60288: 45.0, 60317: 1.0, 61005: 1.0, 61132: 1.0, 61153: 10.0, 61161: 89.0, 61181: 2.0, 61229: 1.0, 61255: 1.0, 61512: 2.0, 61656: 1.0, 62262: 1.0, 62403: 1.0, 62411: 3.0, 62531: 2.0, 62613: 5.0, 62644: 1.0, 62707: 1.0, 62763: 1.0, 62765: 1.0, 62770: 1.0, 63278: 1.0, 63357: 1.0, 63476: 1.0, 63486: 1.0, 63871: 1.0, 63876: 1.0, 63907: 2.0, 64291: 1.0, 64793: 1.0, 64842: 1.0, 64880: 13.0, 64944: 1.0, 64995: 1.0, 65110: 1.0, 65190: 1.0, 65262: 1.0, 65283: 3.0, 65459: 3.0, 65462: 1.0})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.cache()\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Similarity between SOU's\n",
    "#### 50 most stimilar pairs of SOUs given by different Presidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarity(a, b):\n",
    "    '''\n",
    "    Takes:\n",
    "        a, b, lists with [president, year, tfidf vector]\n",
    "    Calculates cosine similarity between tfidf vectors of a and b.'''\n",
    "    return a[2].dot(b[2]) / math.sqrt(a[2].dot(a[2]) * b[2].dot(b[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sou_with_vectors = tfidf.zip(sentences).map(lambda x: [x[1][0], x[1][1], x[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diff_pres = sou_with_vectors.cartesian(sou_with_vectors).filter(lambda x: x[0][0] != x[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diff_pres_sim = diff_pres.map(lambda pair: [pair[0][0], pair[1][0], pair[0][1], pair[1][1],\\\n",
    "                                      similarity(pair[0], pair[1])]).sortBy(lambda tup: tup[4], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['William_J._Clinton', 'Barack_Obama', 1995, 2010, 0.45066786316531809],\n",
       " ['William_J._Clinton', 'Barack_Obama', 1995, 2012, 0.42389028951035135],\n",
       " ['William_J._Clinton', 'Barack_Obama', 1995, 2014, 0.42161066158856031],\n",
       " ['William_J._Clinton', 'Barack_Obama', 1993, 2010, 0.41707423779648189],\n",
       " ['William_J._Clinton', 'Barack_Obama', 1994, 2010, 0.40624974945861375],\n",
       " ['John_Tyler', 'James_K._Polk', 1844, 1846, 0.40439548065401576],\n",
       " ['William_J._Clinton', 'Barack_Obama', 1995, 2016, 0.40016317070440804],\n",
       " ['William_J._Clinton', 'Barack_Obama', 1995, 2011, 0.39846359218367622],\n",
       " ['William_J._Clinton', 'Barack_Obama', 1995, 2015, 0.39402267612637276],\n",
       " ['William_J._Clinton', 'Barack_Obama', 1993, 2011, 0.39257039009946326],\n",
       " ['Andrew_Jackson', 'Martin_Van_Buren', 1836, 1839, 0.39102844024612982],\n",
       " ['William_J._Clinton', 'Barack_Obama', 1994, 2012, 0.39079239535905708],\n",
       " ['William_J._Clinton', 'Barack_Obama', 1993, 2012, 0.38936528292622108],\n",
       " ['William_J._Clinton', 'Barack_Obama', 1993, 2009, 0.3877609431598868],\n",
       " ['William_J._Clinton', 'Barack_Obama', 2000, 2014, 0.38315135276374207],\n",
       " ['William_J._Clinton', 'Barack_Obama', 2000, 2011, 0.38277211846990977],\n",
       " ['William_J._Clinton', 'Barack_Obama', 1994, 2014, 0.38184173299725443],\n",
       " ['John_Tyler', 'James_K._Polk', 1844, 1845, 0.3817660527157079],\n",
       " ['William_J._Clinton', 'Barack_Obama', 2000, 2015, 0.37736473573637158],\n",
       " ['William_J._Clinton', 'Barack_Obama', 2000, 2012, 0.37522963471688447],\n",
       " ['Rutherford_B._Hayes', 'Grover_Cleveland', 1877, 1885, 0.3698020943986311],\n",
       " ['Chester_A._Arthur', 'Grover_Cleveland', 1884, 1885, 0.36940363796788939],\n",
       " ['Grover_Cleveland', 'Benjamin_Harrison', 1885, 1889, 0.36839194211232601],\n",
       " ['Theodore_Roosevelt',\n",
       "  'William_Howard_Taft',\n",
       "  1907,\n",
       "  1912,\n",
       "  0.36433750052814851],\n",
       " ['William_J._Clinton', 'Barack_Obama', 2000, 2010, 0.36257623973578751],\n",
       " ['George_Bush', 'William_J._Clinton', 1992, 1995, 0.36146200126526878],\n",
       " ['William_J._Clinton', 'Barack_Obama', 1993, 2014, 0.36096054796015309],\n",
       " ['William_J._Clinton', 'Barack_Obama', 2000, 2009, 0.36044868859165946],\n",
       " ['Rutherford_B._Hayes', 'Grover_Cleveland', 1880, 1885, 0.35967838696586985],\n",
       " ['Rutherford_B._Hayes', 'Chester_A._Arthur', 1880, 1881, 0.35813139039493658],\n",
       " ['Rutherford_B._Hayes', 'Benjamin_Harrison', 1880, 1889, 0.35779825708380258],\n",
       " ['William_J._Clinton', 'Barack_Obama', 2000, 2016, 0.3575888939053985],\n",
       " ['William_J._Clinton', 'Barack_Obama', 1998, 2011, 0.35735876212976958],\n",
       " ['Theodore_Roosevelt',\n",
       "  'William_Howard_Taft',\n",
       "  1907,\n",
       "  1910,\n",
       "  0.35718359409213318],\n",
       " ['William_J._Clinton', 'Barack_Obama', 1998, 2010, 0.35680667159363166],\n",
       " ['William_J._Clinton', 'Barack_Obama', 1998, 2014, 0.35275139949046613],\n",
       " ['Dwight_D._Eisenhower', 'Jimmy_Carter', 1956, 1981, 0.35075249985377138],\n",
       " ['George_Bush', 'William_J._Clinton', 1989, 2000, 0.35061666699113075],\n",
       " ['Grover_Cleveland', 'Benjamin_Harrison', 1885, 1891, 0.34752841720328509],\n",
       " ['William_J._Clinton', 'Barack_Obama', 1994, 2011, 0.3470244149880668],\n",
       " ['William_J._Clinton', 'Barack_Obama', 1998, 2012, 0.34640897193225462],\n",
       " ['William_J._Clinton', 'George_W._Bush', 2000, 2001, 0.34494984697396891],\n",
       " ['Andrew_Jackson', 'Martin_Van_Buren', 1836, 1837, 0.34412918924787733],\n",
       " ['Benjamin_Harrison', 'Grover_Cleveland', 1891, 1894, 0.34218479185736728],\n",
       " ['Rutherford_B._Hayes', 'Chester_A._Arthur', 1879, 1881, 0.34151284978213631],\n",
       " ['Franklin_Pierce', 'James_Buchanan', 1856, 1858, 0.34135335376349774],\n",
       " ['Calvin_Coolidge', 'Herbert_Hoover', 1927, 1929, 0.3405960689736669],\n",
       " ['William_J._Clinton', 'Barack_Obama', 1995, 2009, 0.33995360444387068],\n",
       " ['James_K._Polk', 'Zachary_Taylor', 1848, 1849, 0.33914902429897076],\n",
       " ['James_K._Polk', 'Millard_Fillmore', 1848, 1851, 0.33909951402115773]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_pres_results = diff_pres_sim.collect()\n",
    "diff_pres_res = [diff_pres_results[i] for i in range(len(diff_pres_results)) if i%2 == 0]\n",
    "diff_pres_res[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50 most similar pairs of SOUs given by the SAME president"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "same_pres = sou_with_vectors.cartesian(sou_with_vectors).filter(lambda x: x[0][0] == x[1][0] and x[0][1] != x[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "same_pres_sim = same_pres.map(lambda pair: [pair[0][0], pair[1][0], pair[0][1], pair[1][1],\\\n",
    "                                      similarity(pair[0], pair[1])]).sortBy(lambda tup: tup[4], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "same_pres_results = same_pres_sim.collect()\n",
    "same_pres_res = [same_pres_results[i] for i in range(len(same_pres_results)) if i%2 == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Barack_Obama', 'Barack_Obama', 2010, 2012, 0.56949171924839703],\n",
       " ['Barack_Obama', 'Barack_Obama', 2010, 2011, 0.55794283726692639],\n",
       " ['George_W._Bush', 'George_W._Bush', 2007, 2008, 0.53262196504098258],\n",
       " ['Barack_Obama', 'Barack_Obama', 2011, 2012, 0.5266175786033126],\n",
       " ['Barack_Obama', 'Barack_Obama', 2010, 2014, 0.52552676298505197],\n",
       " ['Barack_Obama', 'Barack_Obama', 2014, 2015, 0.5253070827361116],\n",
       " ['William_J._Clinton', 'William_J._Clinton', 1999, 2000, 0.52131916078762541],\n",
       " ['William_J._Clinton', 'William_J._Clinton', 1998, 1999, 0.51680263498631673],\n",
       " ['William_McKinley', 'William_McKinley', 1899, 1900, 0.51056803438083176],\n",
       " ['Barack_Obama', 'Barack_Obama', 2012, 2014, 0.50794618875694963],\n",
       " ['William_J._Clinton', 'William_J._Clinton', 1997, 1998, 0.50150381892151141],\n",
       " ['William_J._Clinton', 'William_J._Clinton', 1998, 2000, 0.50046657727222665],\n",
       " ['Barack_Obama', 'Barack_Obama', 2015, 2016, 0.50038638947836089],\n",
       " ['Barack_Obama', 'Barack_Obama', 2010, 2015, 0.49663988172901374],\n",
       " ['James_Buchanan', 'James_Buchanan', 1857, 1858, 0.49469465795940676],\n",
       " ['Barack_Obama', 'Barack_Obama', 2012, 2015, 0.49406085969618202],\n",
       " ['Barack_Obama', 'Barack_Obama', 2009, 2010, 0.48784369130225347],\n",
       " ['William_Howard_Taft',\n",
       "  'William_Howard_Taft',\n",
       "  1910,\n",
       "  1912,\n",
       "  0.48697793835169101],\n",
       " ['Barack_Obama', 'Barack_Obama', 2012, 2016, 0.48492608308971219],\n",
       " ['James_K._Polk', 'James_K._Polk', 1845, 1846, 0.48084567679366441],\n",
       " ['William_J._Clinton', 'William_J._Clinton', 1997, 1999, 0.47814223245253179],\n",
       " ['Barack_Obama', 'Barack_Obama', 2010, 2016, 0.47811357526762249],\n",
       " ['Barack_Obama', 'Barack_Obama', 2011, 2014, 0.47708706547992569],\n",
       " ['Barack_Obama', 'Barack_Obama', 2011, 2015, 0.47516546790350367],\n",
       " ['Barack_Obama', 'Barack_Obama', 2011, 2016, 0.46785683370767345],\n",
       " ['Theodore_Roosevelt', 'Theodore_Roosevelt', 1905, 1907, 0.46622929349290126],\n",
       " ['Dwight_D._Eisenhower',\n",
       "  'Dwight_D._Eisenhower',\n",
       "  1955,\n",
       "  1956,\n",
       "  0.46485384418569758],\n",
       " ['Barack_Obama', 'Barack_Obama', 2014, 2016, 0.46387228143316689],\n",
       " ['James_K._Polk', 'James_K._Polk', 1846, 1847, 0.4612801089039838],\n",
       " ['Andrew_Jackson', 'Andrew_Jackson', 1834, 1835, 0.45801918616129494],\n",
       " ['Theodore_Roosevelt', 'Theodore_Roosevelt', 1907, 1908, 0.45733202193319705],\n",
       " ['Rutherford_B._Hayes',\n",
       "  'Rutherford_B._Hayes',\n",
       "  1879,\n",
       "  1880,\n",
       "  0.45611379428878318],\n",
       " ['Grover_Cleveland', 'Grover_Cleveland', 1893, 1894, 0.45584296775266292],\n",
       " ['William_McKinley', 'William_McKinley', 1898, 1899, 0.45495370357151049],\n",
       " ['James_Buchanan', 'James_Buchanan', 1859, 1860, 0.45413021587408575],\n",
       " ['William_Howard_Taft',\n",
       "  'William_Howard_Taft',\n",
       "  1911,\n",
       "  1912,\n",
       "  0.45312490676464484],\n",
       " ['Barack_Obama', 'Barack_Obama', 2009, 2012, 0.45257752205008006],\n",
       " ['William_J._Clinton', 'William_J._Clinton', 1997, 2000, 0.45105585246626023],\n",
       " ['William_J._Clinton', 'William_J._Clinton', 1994, 1995, 0.44858741605334723],\n",
       " ['Barack_Obama', 'Barack_Obama', 2012, 2013, 0.44815526054342331],\n",
       " ['James_Buchanan', 'James_Buchanan', 1858, 1860, 0.44774004029045217],\n",
       " ['Grover_Cleveland', 'Grover_Cleveland', 1885, 1886, 0.44699257054242225],\n",
       " ['Theodore_Roosevelt', 'Theodore_Roosevelt', 1904, 1905, 0.44691415960668762],\n",
       " ['Lyndon_B._Johnson', 'Lyndon_B._Johnson', 1966, 1967, 0.44537530732094077],\n",
       " ['George_W._Bush', 'George_W._Bush', 2005, 2008, 0.4452836919658108],\n",
       " ['William_Howard_Taft',\n",
       "  'William_Howard_Taft',\n",
       "  1909,\n",
       "  1910,\n",
       "  0.44028093665750173],\n",
       " ['James_Buchanan', 'James_Buchanan', 1858, 1859, 0.43771981052588493],\n",
       " ['William_J._Clinton', 'William_J._Clinton', 1993, 1994, 0.43398548486673399],\n",
       " ['Barack_Obama', 'Barack_Obama', 2009, 2011, 0.43302723476805061],\n",
       " ['Theodore_Roosevelt', 'Theodore_Roosevelt', 1906, 1907, 0.43121571803068776]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same_pres_res[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 25 most similar pairs of PRESIDENTS, averaging similarity over their SOUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum_pres = sc.parallelize(diff_pres_res).map(lambda x: [(x[0], x[1]), (x[4], 1)]).reduceByKey(\\\n",
    "            lambda x, y: [x[0] + y[0], x[1] + y[1]]).map(lambda x: [x[0], x[1][0] / x[1][1]]).sortBy(\\\n",
    "            lambda x: x[1], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('William_J._Clinton', 'Barack_Obama'), 0.32047043952494458],\n",
       " [('Zachary_Taylor', 'Millard_Fillmore'), 0.3091098445897616],\n",
       " [('George_Bush', 'William_J._Clinton'), 0.28355830862776432],\n",
       " [('James_K._Polk', 'Zachary_Taylor'), 0.28175776742067576],\n",
       " [('James_K._Polk', 'Millard_Fillmore'), 0.27863443896053469],\n",
       " [('Benjamin_Harrison', 'Grover_Cleveland'), 0.27531318650763553],\n",
       " [('Andrew_Jackson', 'Martin_Van_Buren'), 0.27487662304426386],\n",
       " [('Theodore_Roosevelt', 'William_Howard_Taft'), 0.26940030231243306],\n",
       " [('Rutherford_B._Hayes', 'Chester_A._Arthur'), 0.2693862051761256],\n",
       " [('George_Bush', 'Barack_Obama'), 0.26320806458339235],\n",
       " [('John_Tyler', 'James_K._Polk'), 0.26162609074712229],\n",
       " [('Rutherford_B._Hayes', 'Benjamin_Harrison'), 0.25691746463199117],\n",
       " [('Rutherford_B._Hayes', 'Grover_Cleveland'), 0.2541275401483995],\n",
       " [('Martin_Van_Buren', 'John_Tyler'), 0.25241087282692637],\n",
       " [('James_K._Polk', 'James_Buchanan'), 0.24765476504605999],\n",
       " [('Millard_Fillmore', 'Franklin_Pierce'), 0.24672230498639983],\n",
       " [('Grover_Cleveland', 'Benjamin_Harrison'), 0.24642939499085162],\n",
       " [('William_McKinley', 'William_Howard_Taft'), 0.24599679402761224],\n",
       " [('Franklin_Pierce', 'James_Buchanan'), 0.24597224003773135],\n",
       " [('Zachary_Taylor', 'Franklin_Pierce'), 0.24544832982134335],\n",
       " [('Ronald_Reagan', 'William_J._Clinton'), 0.24471846297548819],\n",
       " [('Chester_A._Arthur', 'Benjamin_Harrison'), 0.24448746045106512],\n",
       " [('Chester_A._Arthur', 'Grover_Cleveland'), 0.24350051695509548],\n",
       " [('Martin_Van_Buren', 'James_K._Polk'), 0.24106694195965017],\n",
       " [('Andrew_Jackson', 'Millard_Fillmore'), 0.24068197115332093]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_pres.take(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Comments on findings:\n",
    "\n",
    "It's hard to tell if so many speeches are similar by reading them, as they are significantly lengthy documents. Even the online speeches are long (e.g. Clinton's 1995 speech on YouTube is 1h25). Tyler and Polk do speak A LOT about Mexico; which makes sense because of the Mexican-American War that ocurred after Texas annexation to the US in that period. But judging similarity of so many speeches would require several hours to read all of them.\n",
    "\n",
    "Nevertheless, regarding a better measure of \"similarity\", one could try:\n",
    "- Measuring common topics of speeches (even if they don't use the exact same words) using techniques like word2vec.\n",
    "- Finding similar grammatical patterns using a Naive Bayes model. This would mean training a model with speeches from one president and then calculating how likely it is that another president or speech was produced with the training data. \n",
    "- Finally, taking into account sentiment of the speech. \n",
    "\n",
    "There might be a way to combine all of these into a single pipeline?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Cluster the speeches using k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "some = sou_with_vectors.map(lambda x: x[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the \"cost\" (i.e. empirical risk) of each clustering. Let's take then the marginal decrease in risk, to see how much we're improving with each extra cluster.\n",
    "\n",
    "(Note: Notice how the risk seems to be non-increasing as k increases. Unfortunately, there are times where risk actually goes up. This is due to the initialization of clusters, and the fact that the algorithms returns local minima.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using random initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1, cost: 4,542,241\n",
      "k = 2, cost: 4,518,591\n",
      "k = 3, cost: 4,264,491\n",
      "k = 4, cost: 4,158,019\n",
      "k = 5, cost: 3,910,926\n",
      "k = 6, cost: 3,875,895\n",
      "k = 7, cost: 3,755,000\n",
      "k = 8, cost: 3,552,139\n",
      "k = 9, cost: 3,719,476\n",
      "k = 10, cost: 3,578,208\n"
     ]
    }
   ],
   "source": [
    "rand_res = []\n",
    "for i in range(1,11):\n",
    "    clusters = KMeans.train(some, i, maxIterations=50, runs=10, seed=1, initializationMode='random')\n",
    "    cost = clusters.computeCost(some)\n",
    "    print(\"k = {0}, cost: {1:,.0f}\".format(i, cost))\n",
    "    rand_res.append(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1: -23,649\n",
      "k=2: -254,101\n",
      "k=3: -106,471\n",
      "k=4: -247,093\n",
      "k=5: -35,031\n",
      "k=6: -120,895\n",
      "k=7: -202,861\n",
      "k=8: 167,337\n",
      "k=9: -141,268\n"
     ]
    }
   ],
   "source": [
    "for x in range(1,len(rand_res)):\n",
    "    print(\"k={0}: {1:,.0f}\".format(x, rand_res[x]-rand_res[x-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using k-means++ initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1, cost: 4,542,241\n",
      "k = 2, cost: 4,431,084\n",
      "k = 3, cost: 4,137,085\n",
      "k = 4, cost: 3,879,967\n",
      "k = 5, cost: 3,805,591\n",
      "k = 6, cost: 3,711,384\n",
      "k = 7, cost: 3,420,000\n",
      "k = 8, cost: 3,544,678\n",
      "k = 9, cost: 3,355,572\n",
      "k = 10, cost: 3,322,908\n"
     ]
    }
   ],
   "source": [
    "km_res = []\n",
    "for i in range(1,11):\n",
    "    clusters = KMeans.train(some, i, maxIterations=50, runs=10, seed=1, initializationMode='k-means||')\n",
    "    cost = clusters.computeCost(some)\n",
    "    print(\"k = {0}, cost: {1:,.0f}\".format(i, cost))\n",
    "    km_res.append(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1: -111,157\n",
      "k=2: -293,998\n",
      "k=3: -257,119\n",
      "k=4: -74,376\n",
      "k=5: -94,207\n",
      "k=6: -291,384\n",
      "k=7: 124,678\n",
      "k=8: -189,105\n",
      "k=9: -32,665\n"
     ]
    }
   ],
   "source": [
    "for x in range(1,len(km_res)):\n",
    "    print(\"k={0}: {1:,.0f}\".format(x, km_res[x]-km_res[x-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would seem that initializing the clustering with k-means++ yields a lower error than random assignment. This might be useful, but we should be careful not to overfit our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 clusters, difference of 0\n",
      "1 clusters, difference of 87,508\n",
      "2 clusters, difference of 127,405\n",
      "3 clusters, difference of 278,053\n",
      "4 clusters, difference of 105,335\n",
      "5 clusters, difference of 164,512\n",
      "6 clusters, difference of 335,000\n",
      "7 clusters, difference of 7,461\n",
      "8 clusters, difference of 363,904\n",
      "9 clusters, difference of 255,300\n"
     ]
    }
   ],
   "source": [
    "for i, num in enumerate(np.array(rand_res) - np.array(km_res)):\n",
    "    print(\"{0} clusters, difference of {1:,.0f}\".format(i, num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Comments:\n",
    "\n",
    "The problems with these clusters is that they live in a significantly high-dimensional space, so it's hard to interpret them. We could take the top 20 TF-IDF values of each one and see if we can get any insights on the documents they represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
